{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<p style=\"align: center;\"><img align=center src=\"https://drive.google.com/uc?export=view&id=1I8kDikouqpH4hf7JBiSYAeNT2IO52T-T\" width=600 height=480/></p>\n<h3 style=\"text-align: center;\"><b>Школа глубокого обучения ФПМИ МФТИ</b></h3>\n\n<h3 style=\"text-align: center;\"><b>Домашнее задание. Весна 2021</b></h3>\n\n# Autoencoders\n","metadata":{"id":"LQ7i1HkmYY68"}},{"cell_type":"markdown","source":"# Часть 1. Vanilla Autoencoder (10 баллов)","metadata":{"id":"Wru2LNFuL2Iq"}},{"cell_type":"markdown","source":"## 1.1. Подготовка данных (0.5 балла)\n","metadata":{"id":"kr3STtdpYY7G"}},{"cell_type":"code","source":"import numpy as np\nfrom torch.autograd import Variable\nfrom torchvision import datasets\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.utils.data as data_utils\nimport torch\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline","metadata":{"id":"xTNi9JLRYY7I","execution":{"iopub.status.busy":"2022-06-02T14:55:08.615173Z","iopub.execute_input":"2022-06-02T14:55:08.615585Z","iopub.status.idle":"2022-06-02T14:55:11.383063Z","shell.execute_reply.started":"2022-06-02T14:55:08.615515Z","shell.execute_reply":"2022-06-02T14:55:11.382338Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"def fetch_dataset(attrs_name = \"lfw_attributes.txt\",\n                      images_name = \"lfw-deepfunneled\",\n                      dx=80,dy=80,\n                      dimx=64,dimy=64\n    ):\n\n    #download if not exists\n    if not os.path.exists(images_name):\n        print(\"images not found, donwloading...\")\n        os.system(\"wget http://vis-www.cs.umass.edu/lfw/lfw-deepfunneled.tgz -O tmp.tgz\")\n        print(\"extracting...\")\n        os.system(\"tar xvzf tmp.tgz && rm tmp.tgz\")\n        print(\"done\")\n        assert os.path.exists(images_name)\n\n    if not os.path.exists(attrs_name):\n        print(\"attributes not found, downloading...\")\n        os.system(\"wget http://www.cs.columbia.edu/CAVE/databases/pubfig/download/%s\" % attrs_name)\n        print(\"done\")\n\n    #read attrs\n    df_attrs = pd.read_csv(\"lfw_attributes.txt\",sep='\\t',skiprows=1,) \n    df_attrs = pd.DataFrame(df_attrs.iloc[:,:-1].values, columns = df_attrs.columns[1:])\n\n\n    #read photos\n    photo_ids = []\n    for dirpath, dirnames, filenames in os.walk(images_name):\n        for fname in filenames:\n            if fname.endswith(\".jpg\"):\n                fpath = os.path.join(dirpath,fname)\n                photo_id = fname[:-4].replace('_',' ').split()\n                person_id = ' '.join(photo_id[:-1])\n                photo_number = int(photo_id[-1])\n                photo_ids.append({'person':person_id,'imagenum':photo_number,'photo_path':fpath})\n\n    photo_ids = pd.DataFrame(photo_ids)\n    # print(photo_ids)\n    #mass-merge\n    #(photos now have same order as attributes)\n    df = pd.merge(df_attrs,photo_ids,on=('person','imagenum'))\n\n    assert len(df)==len(df_attrs),\"lost some data when merging dataframes\"\n\n    # print(df.shape)\n    #image preprocessing\n    all_photos =df['photo_path'].apply(skimage.io.imread)\\\n                                .apply(lambda img:img[dy:-dy,dx:-dx])\\\n                                .apply(lambda img: resize(img,[dimx,dimy]))\n\n    all_photos = np.stack(all_photos.values)#.astype('uint8')\n    all_attrs = df.drop([\"photo_path\",\"person\",\"imagenum\"],axis=1)\n    \n    return all_photos, all_attrs","metadata":{"id":"zvAjov5F2NvE","execution":{"iopub.status.busy":"2022-06-02T14:55:11.384766Z","iopub.execute_input":"2022-06-02T14:55:11.385007Z","iopub.status.idle":"2022-06-02T14:55:11.397845Z","shell.execute_reply.started":"2022-06-02T14:55:11.384974Z","shell.execute_reply":"2022-06-02T14:55:11.396871Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# The following line fetches you two datasets: images, usable for autoencoder training and attributes.\n# Those attributes will be required for the final part of the assignment (applying smiles), so please keep them in mind\n\n#from datasets import fetch_dataset\nimport os\nimport pandas as pd\nimport skimage.io\nfrom skimage.transform import resize\ndata, attrs = fetch_dataset()","metadata":{"id":"W3KhlblLYY7P","execution":{"iopub.status.busy":"2022-06-02T14:55:15.105012Z","iopub.execute_input":"2022-06-02T14:55:15.105757Z","iopub.status.idle":"2022-06-02T14:56:05.335692Z","shell.execute_reply.started":"2022-06-02T14:55:15.105719Z","shell.execute_reply":"2022-06-02T14:56:05.334840Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"\nРазбейте выборку картинок на train и val, выведите несколько картинок в output, чтобы посмотреть, как они выглядят, и приведите картинки к тензорам pytorch, чтобы можно было скормить их сети:","metadata":{"id":"8MSzXXGoYY7X"}},{"cell_type":"code","source":"train_photos, val_photos, train_attrs, val_attrs = train_test_split(data, attrs,\n                                                                    train_size=0.9, shuffle=True)\ntrain_loader = torch.utils.data.DataLoader(train_photos, batch_size=32)\nval_loader = torch.utils.data.DataLoader(val_photos, batch_size=32)\nn = 5\ngenphoto = np.random.choice(len(train_photos), n)\ngenphot = [i for i in genphoto]\nlst = []\nfor i, j in zip(range(n), genphoto):\n    plt.figure()\n    #plt.imshow(X_train[j]) \n    plt.imshow(train_photos[j])","metadata":{"execution":{"iopub.status.busy":"2022-06-02T14:56:05.339271Z","iopub.execute_input":"2022-06-02T14:56:05.339484Z","iopub.status.idle":"2022-06-02T14:56:06.425215Z","shell.execute_reply.started":"2022-06-02T14:56:05.339458Z","shell.execute_reply":"2022-06-02T14:56:06.424470Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## 1.2. Архитектура модели (1.5 балла)\nВ этом разделе мы напишем и обучем обычный автоэнкодер.\n\n\n\n<img src=\"https://www.notion.so/image/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F4b8adf79-8e6a-4b7d-9061-8617a00edbb1%2F__2021-04-30__14.53.33.png?table=block&id=56f187b4-279f-4208-b1ed-4bda5f91bfc0&width=2880&userId=3b1b5e32-1cfb-4b0f-8705-5a524a8f56e3&cache=v2\" alt=\"Autoencoder\">\n\n\n^ напомню, что автоэнкодер выглядит вот так","metadata":{"id":"z9CC-DUhYY7i"}},{"cell_type":"code","source":"dim_code = 75 # выберите размер латентного вектора","metadata":{"id":"csrNCYh-YY7j","execution":{"iopub.status.busy":"2022-06-02T14:56:19.298391Z","iopub.execute_input":"2022-06-02T14:56:19.298704Z","iopub.status.idle":"2022-06-02T14:56:19.302986Z","shell.execute_reply.started":"2022-06-02T14:56:19.298670Z","shell.execute_reply":"2022-06-02T14:56:19.302015Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Реализуем autoencoder. Архитектуру (conv, fully-connected, ReLu, etc) можете выбирать сами. Экспериментируйте!","metadata":{"id":"Fjr-N8AWee-k"}},{"cell_type":"code","source":"from copy import deepcopy\n\nclass Autoencoder(nn.Module):\n    def __init__(self):\n        super(Autoencoder, self).__init__()\n        self.flatten = nn.Flatten()\n        self.encoder = nn.Sequential(\n        nn.Linear(12288, 512),\n        nn.ReLU(),\n        nn.Linear(512, 256),\n        nn.ReLU(),\n        nn.Linear(256, 128),\n        nn.ReLU(),\n        nn.Linear(128, dim_code))\n        \n        self.decoder = nn.Sequential(\n        nn.Linear(dim_code, 128),\n        nn.ReLU(),\n        nn.Linear(128, 256),\n        nn.ReLU(),\n        nn.Linear(256, 512),\n        nn.ReLU(),\n        nn.Linear(512, 12288),\n        nn.ReLU())\n        \n        \n    def forward(self, x, z):\n        \n        #<реализуйте forward проход автоэнкодера\n        #в качестве ваозвращаемых переменных -- латентное представление картинки (latent_code) \n        #и полученная реконструкция изображения (reconstruction)>\n        \n        #return reconstruction, latent_code\n        x = self.flatten(x).float()\n        return self.decoder(self.encoder(x)), self.encoder(x), self.decoder(z)\n","metadata":{"id":"8SjHNX-rYY7k","execution":{"iopub.status.busy":"2022-06-02T14:56:22.834814Z","iopub.execute_input":"2022-06-02T14:56:22.835520Z","iopub.status.idle":"2022-06-02T14:56:22.847631Z","shell.execute_reply.started":"2022-06-02T14:56:22.835481Z","shell.execute_reply":"2022-06-02T14:56:22.846653Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"criterion = nn.MSELoss()\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\nautoencoder = Autoencoder().to(device)\n\noptimizer = torch.optim.Adam(autoencoder.parameters())\n#print(autoencoder)","metadata":{"id":"73lg3bI2YY7m","execution":{"iopub.status.busy":"2022-06-02T14:56:24.180421Z","iopub.execute_input":"2022-06-02T14:56:24.181120Z","iopub.status.idle":"2022-06-02T14:56:26.526909Z","shell.execute_reply.started":"2022-06-02T14:56:24.181083Z","shell.execute_reply":"2022-06-02T14:56:26.526135Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"## 1.3 Обучение (2 балла)","metadata":{"id":"GpntmZCe5L6i"}},{"cell_type":"markdown","source":"Осталось написать код обучения автоэнкодера. При этом было бы неплохо в процессе иногда смотреть, как автоэнкодер реконструирует изображения на данном этапе обучения. Наример, после каждой эпохи (прогона train выборки через автоэекодер) можно смотреть, какие реконструкции получились для каких-то изображений val выборки.\n\nА, ну еще было бы неплохо выводить графики train и val лоссов в процессе тренировки =)","metadata":{"id":"Bdxg_3WJYY7o"}},{"cell_type":"code","source":"train_photos, val_photos, train_attrs, val_attrs = train_test_split(data, attrs,\n                                                                    train_size=0.9, shuffle=True)\ntrain_loader = torch.utils.data.DataLoader(train_photos, batch_size=32)\nval_loader = torch.utils.data.DataLoader(val_photos, batch_size=32)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T14:56:26.945428Z","iopub.execute_input":"2022-06-02T14:56:26.946131Z","iopub.status.idle":"2022-06-02T14:56:27.345946Z","shell.execute_reply.started":"2022-06-02T14:56:26.946094Z","shell.execute_reply":"2022-06-02T14:56:27.345143Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"n_epochs = 25\ntrain_losses = []\nval_losses = []\nlatent_lst = []\nz = torch.tensor(np.random.randn(25, dim_code))\n\ndef my_plot(epochs, loss):\n    plt.plot(epochs, loss)\n    \n    \nfor epoch in tqdm(range(n_epochs)):\n    autoencoder.train()\n    train_losses_per_epoch = []\n    for batch in train_loader:\n        #print(batch.to(device).shape)\n        optimizer.zero_grad()\n        reconstruction, latent_code, z_dist = autoencoder(batch.to(device), z.to(device).float())\n        latent_lst.extend(latent_code)\n        #print(reconstruction.shape)\n        reconstruction = reconstruction.view(-1, 64, 64, 3)\n        #print(reconstruction.shape)\n        loss = criterion(batch.to(device).float(), reconstruction)\n        loss.backward()\n        optimizer.step()\n        train_losses_per_epoch.append(loss.item())\n        #writer.add_scalar('Loss/Train', loss, epoch)\n    train_losses.append(np.mean(train_losses_per_epoch))\n\n    autoencoder.eval()\n    val_losses_per_epoch = []\n    with torch.no_grad():\n        for batch in val_loader:\n            reconstruction, latent_code, z_val = autoencoder(batch.to(device),z.to(device).float() )\n            reconstruction = reconstruction.view(-1, 64, 64, 3)\n            loss = criterion(batch.to(device).float(), reconstruction)\n            val_losses_per_epoch.append(loss.item())\n            #writer.add_scalar('Loss/Validation', loss, epoch)\n    val_losses.append(np.mean(val_losses_per_epoch))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T14:56:42.842678Z","iopub.execute_input":"2022-06-02T14:56:42.843172Z","iopub.status.idle":"2022-06-02T14:57:54.010182Z","shell.execute_reply.started":"2022-06-02T14:56:42.843056Z","shell.execute_reply":"2022-06-02T14:57:54.009468Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"epochs__ = range(1,26)\nplt.plot(epochs__, train_losses, 'g', label='Training loss')\nplt.plot(epochs__, val_losses, 'b', label='validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T14:59:37.145101Z","iopub.execute_input":"2022-06-02T14:59:37.146239Z","iopub.status.idle":"2022-06-02T14:59:37.348120Z","shell.execute_reply.started":"2022-06-02T14:59:37.146183Z","shell.execute_reply":"2022-06-02T14:59:37.347411Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"Давайте посмотрим, как наш тренированный автоэекодер кодирует и восстанавливает картинки:","metadata":{"id":"FAztAMA4YY7q"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\n\nautoencoder.eval()\nwith torch.no_grad():\n    for batch in val_loader:\n        reconstruction, laten_code, zdis = autoencoder(batch.to(device), z.to(device).float())\n        reconstruction = reconstruction.view(  -1, 64, 64, 3)\n        result = reconstruction.cpu().detach().numpy()\n        ground_truth = batch.numpy()\n        break\n        \n        \n        \nplt.figure(figsize=(8, 20))\nfor i, (gt, res) in enumerate(zip(ground_truth[:10], result[:10])):\n    #print(ground_truth[:10])\n    plt.subplot(10, 2, 2*i+1)\n    plt.imshow(gt)\n    plt.subplot(10, 2, 2*i+2)\n    plt.imshow(res)","metadata":{"id":"I1J__yvxYY7r","execution":{"iopub.status.busy":"2022-06-02T15:00:22.397730Z","iopub.execute_input":"2022-06-02T15:00:22.398394Z","iopub.status.idle":"2022-06-02T15:00:24.133008Z","shell.execute_reply.started":"2022-06-02T15:00:22.398350Z","shell.execute_reply":"2022-06-02T15:00:24.129927Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"Not bad, right? ","metadata":{"id":"1OPh9O6UYY7s"}},{"cell_type":"markdown","source":"## 1.4. Sampling (2 балла)","metadata":{"id":"dFi96giuYY7t"}},{"cell_type":"markdown","source":"Давайте теперь будем не просто брать картинку, прогонять ее через автоэекодер и получать реконструкцию, а попробуем создать что-то НОВОЕ\n\nДавайте возьмем и подсунем декодеру какие-нибудь сгенерированные нами векторы (например, из нормального распределения) и посмотрим на результат реконструкции декодера:\n\n__Подсказка:__Е сли вместо лиц у вас выводится непонятно что, попробуйте посмотреть, как выглядят латентные векторы картинок из датасета. Так как в обучении нейронных сетей есть определенная доля рандома, векторы латентного слоя могут быть распределены НЕ как `np.random.randn(25, <latent_space_dim>)`. А чтобы у нас получались лица при запихивании вектора декодеру, вектор должен быть распределен так же, как латентные векторы реальных фоток. Так что в таком случае придется рандом немного подогнать.","metadata":{"id":"AOtUaPNYYY7t"}},{"cell_type":"code","source":"encod  = []\nz = torch.tensor(np.random.randn(25, dim_code))\nwith torch.no_grad():\n    for idx, batch in enumerate(train_loader):\n        batch = batch.view( batch.size(0), -1)\n        #print(batch.shape)\n        reconstruction = autoencoder.encoder(batch.to(device).float())\n        #reconstruction = reconstruction.view(-1, 64, 64, 3)\n        res_z = reconstruction.cpu().detach().numpy()\n        ground_truth = batch.numpy()\n        encod.extend(res_z)\n        #reck.append(res_z)\nencod = np.array(encod).T","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:00:42.899541Z","iopub.execute_input":"2022-06-02T15:00:42.899830Z","iopub.status.idle":"2022-06-02T15:00:43.515362Z","shell.execute_reply.started":"2022-06-02T15:00:42.899798Z","shell.execute_reply":"2022-06-02T15:00:43.514666Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"z_mean = np.mean([encod[i].mean() for i in range(len(encod))])\nz_std = np.mean([encod[i].std() for i in range(len(encod))])\nprint(z_mean, z_std)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:00:45.243868Z","iopub.execute_input":"2022-06-02T15:00:45.244130Z","iopub.status.idle":"2022-06-02T15:00:45.268017Z","shell.execute_reply.started":"2022-06-02T15:00:45.244101Z","shell.execute_reply":"2022-06-02T15:00:45.267287Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"z = torch.normal(z_mean, 3*z_std, size = (25, dim_code)) #-0.18094799, 0.505032733\nprint(z.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:00:47.919425Z","iopub.execute_input":"2022-06-02T15:00:47.920210Z","iopub.status.idle":"2022-06-02T15:00:47.929102Z","shell.execute_reply.started":"2022-06-02T15:00:47.920164Z","shell.execute_reply":"2022-06-02T15:00:47.928351Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"autoencoder.eval()\nreck = []\ngro = []\nwith torch.no_grad():\n    for idx, batch in enumerate(z):\n        #print(batch.size())\n        reconstruction = autoencoder.decoder(batch.to(device).float())\n        reconstruction = reconstruction.view(-1, 64, 64, 3)\n        res_z = reconstruction.cpu().detach().numpy()\n        ground_truth = batch.numpy()\n        reck.append(res_z)\n        gro.append(ground_truth)\nlstim = []\nplt.figure(figsize=(8, 20))\nfor i, res in enumerate(reck[:15]):\n    plt.subplot(15, 2, 2*i+1)\n    plt.imshow(res[0])\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:04:35.041851Z","iopub.execute_input":"2022-06-02T15:04:35.042439Z","iopub.status.idle":"2022-06-02T15:04:36.269498Z","shell.execute_reply.started":"2022-06-02T15:04:35.042400Z","shell.execute_reply":"2022-06-02T15:04:36.268858Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"## Time to make fun! (4 балла)\n\nДавайте научимся пририсовывать людям улыбки =)","metadata":{"id":"Ey8dD9s0YY7w"}},{"cell_type":"markdown","source":"<img src=\"https://i.imgur.com/tOE9rDK.png\" alt=\"linear\" width=\"700\" height=\"400\">","metadata":{"id":"i1v-8WwuYY7w"}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"План такой:\n\n1. Нужно выделить \"вектор улыбки\": для этого нужно из выборки изображений найти несколько (~15) людей с улыбками и столько же без.\n\nНайти людей с улыбками вам поможет файл с описанием датасета, скачанный вместе с датасетом. В нем указаны имена картинок и присутствубщие атрибуты (улыбки, очки...)\n\n2. Вычислить латентный вектор для всех улыбающихся людей (прогнать их через encoder) и то же для всех грустненьких\n\n3. Вычислить, собственно, вектор улыбки -- посчитать разность между средним латентным вектором улыбающихся людей и средним латентным вектором грустных людей\n\n4. А теперь приделаем улыбку грустному человеку: добавим полученный в пункте 3 вектор к латентному вектору грустного человека и прогоним полученный вектор через decoder. Получим того же человека, но уже не грустненького!","metadata":{"id":"eGE0M2GDYY7x"}},{"cell_type":"code","source":"smile = sorted(train_attrs['Smiling'])[len(train_attrs)-45:] # вместо 15 взял 45 изображений\nsmile_not = sorted(train_attrs['Smiling'])[2555:2600] \n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:21:44.436100Z","iopub.execute_input":"2022-06-02T15:21:44.436356Z","iopub.status.idle":"2022-06-02T15:21:44.450586Z","shell.execute_reply.started":"2022-06-02T15:21:44.436326Z","shell.execute_reply":"2022-06-02T15:21:44.449532Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"smile_photo = []\nsmilenot_photo = []\nfor i, j in enumerate(train_attrs['Smiling']):\n    if j in smile:\n        smile_photo.append(i)\n    if j in smile_not:\n        smilenot_photo.append(i)\n        ","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:21:46.978395Z","iopub.execute_input":"2022-06-02T15:21:46.979006Z","iopub.status.idle":"2022-06-02T15:21:46.998862Z","shell.execute_reply.started":"2022-06-02T15:21:46.978966Z","shell.execute_reply":"2022-06-02T15:21:46.997953Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"print(smilenot_photo)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:08:23.179101Z","iopub.execute_input":"2022-06-02T15:08:23.179528Z","iopub.status.idle":"2022-06-02T15:08:23.187576Z","shell.execute_reply.started":"2022-06-02T15:08:23.179487Z","shell.execute_reply":"2022-06-02T15:08:23.184183Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"plt.imshow(train_photos[144])","metadata":{"execution":{"iopub.status.busy":"2022-05-25T23:25:50.139502Z","iopub.execute_input":"2022-05-25T23:25:50.139788Z","iopub.status.idle":"2022-05-25T23:25:50.331659Z","shell.execute_reply.started":"2022-05-25T23:25:50.139752Z","shell.execute_reply":"2022-05-25T23:25:50.330946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print(smile_photo)\nencod_smile = []\nencod_smilenot = []\n#encod_smile = np.array(enc)\ntrain_photos = torch.tensor(train_photos)\nwith torch.no_grad():\n    for idx, nidx in zip(smile_photo, smilenot_photo):\n        reconstruction_smile = autoencoder.encoder(torch.tensor(train_photos[idx].view(1, 64*64*3)).to(device).float())\n        reconstruction_smilenot = autoencoder.encoder(torch.tensor(train_photos[nidx].view(1, 64*64*3)).to(device).float())   \n        encod_smile.extend(reconstruction_smile.cpu().detach().numpy())\n        encod_smilenot.extend(reconstruction_smilenot.cpu().detach().numpy())\n        ","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:23:30.467809Z","iopub.execute_input":"2022-06-02T15:23:30.468078Z","iopub.status.idle":"2022-06-02T15:23:30.751363Z","shell.execute_reply.started":"2022-06-02T15:23:30.468048Z","shell.execute_reply":"2022-06-02T15:23:30.750650Z"},"trusted":true},"execution_count":83,"outputs":[]},{"cell_type":"code","source":"fortrain_smile = encod_smile\nfortrain_smilenot = encod_smilenot","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:23:40.994099Z","iopub.execute_input":"2022-06-02T15:23:40.994373Z","iopub.status.idle":"2022-06-02T15:23:40.998319Z","shell.execute_reply.started":"2022-06-02T15:23:40.994344Z","shell.execute_reply":"2022-06-02T15:23:40.997638Z"},"trusted":true},"execution_count":84,"outputs":[]},{"cell_type":"code","source":"encod_smile, encod_smilenot = np.array(encod_smile).T, np.array(encod_smilenot).T","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:25:25.073935Z","iopub.execute_input":"2022-06-02T15:25:25.076821Z","iopub.status.idle":"2022-06-02T15:25:25.081865Z","shell.execute_reply.started":"2022-06-02T15:25:25.076771Z","shell.execute_reply":"2022-06-02T15:25:25.080850Z"},"trusted":true},"execution_count":87,"outputs":[]},{"cell_type":"code","source":"encod_smile_new = [encod_smile[i].mean() for i in range(len(encod_smile))]\nencod_smilenot_new = [encod_smilenot[i].mean() for i in range(len(encod_smilenot))]\nnew_smile = [encod_smile_new[i] - encod_smilenot_new[i] for i in range(dim_code)]\nfortrain_smilenot_ = []\nfor i in range(len(fortrain_smilenot)):\n    fortrain_smilenot_.append(fortrain_smilenot[i] + 2 * new_smile[i])","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:25:48.916288Z","iopub.execute_input":"2022-06-02T15:25:48.916546Z","iopub.status.idle":"2022-06-02T15:25:48.924740Z","shell.execute_reply.started":"2022-06-02T15:25:48.916510Z","shell.execute_reply":"2022-06-02T15:25:48.923683Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"for_smile = []\nfor_smile_not = []\nwith torch.no_grad():\n    for vector, idx in zip(fortrain_smilenot_, smilenot_photo):\n        vec_recon = autoencoder.decoder(torch.tensor(vector).to(device).float())\n        vec_recon = vec_recon.view(-1, 64, 64, 3).cpu().detach().numpy()\n        for_smile.extend(vec_recon)\n        sm_not, enc, dec = autoencoder(torch.tensor(train_photos[idx].view(1, 64*64*3)).to(device).float(), z.to(device).float())\n        sm_not = sm_not.view(-1, 64, 64, 3).cpu().detach().numpy()\n        for_smile_not.extend(sm_not)\n        \n\nplt.figure(figsize=(8, 20))\nfor i, (res1, res2) in enumerate(zip(for_smile[:15], for_smile_not[:15])):\n    plt.subplot(15, 2, 2*i+1)\n    plt.imshow(res2)\n    plt.subplot(15, 2, 2*i+2)\n    plt.imshow(res1)\n\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:25:51.185358Z","iopub.execute_input":"2022-06-02T15:25:51.185953Z","iopub.status.idle":"2022-06-02T15:25:53.435375Z","shell.execute_reply.started":"2022-06-02T15:25:51.185914Z","shell.execute_reply":"2022-06-02T15:25:53.434461Z"},"trusted":true},"execution_count":92,"outputs":[]},{"cell_type":"markdown","source":"Вуаля! Вы восхитительны!","metadata":{"id":"bXI6jprOYY7z"}},{"cell_type":"markdown","source":"Теперь вы можете пририсовывать людям не только улыбки, но и много чего другого -- закрывать/открывать глаза, пририсовывать очки... в общем, все, на что хватит фантазии и на что есть атрибуты в `all_attrs`:)","metadata":{"id":"E2UAf0bpYY70"}},{"cell_type":"markdown","source":"# Часть 2: Variational Autoencoder (10 баллов) ","metadata":{"id":"QQnEGmknYY71"}},{"cell_type":"markdown","source":"Займемся обучением вариационных автоэнкодеров — проапгрейженной версии AE. Обучать будем на датасете MNIST, содержащем написанные от руки цифры от 0 до 9","metadata":{"id":"bWQNRjJq2uTz"}},{"cell_type":"code","source":"from torchvision.transforms import *\nbatch_size = 32\n# MNIST Dataset\ntrain_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\ntest_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"id":"qBXXr9njByYC","execution":{"iopub.status.busy":"2022-06-02T15:30:05.282917Z","iopub.execute_input":"2022-06-02T15:30:05.283176Z","iopub.status.idle":"2022-06-02T15:30:06.982795Z","shell.execute_reply.started":"2022-06-02T15:30:05.283148Z","shell.execute_reply":"2022-06-02T15:30:06.982004Z"},"trusted":true},"execution_count":93,"outputs":[]},{"cell_type":"markdown","source":"## 2.1 Архитектура модели и обучение (2 балла)\n\nРеализуем VAE. Архитектуру (conv, fully-connected, ReLu, etc) можете выбирать сами. Рекомендуем пользоваться более сложными моделями, чем та, что была на семинаре:) Экспериментируйте!","metadata":{"id":"2rHphW5l8Wgi"}},{"cell_type":"code","source":"class Reshape(nn.Module):\n    def __init__(self, *args):\n        super().__init__()\n        self.shape = args\n\n    def forward(self, x):\n        return x.view(self.shape)\n\n\nclass Trim(nn.Module):\n    def __init__(self, *args):\n        super().__init__()\n\n    def forward(self, x):\n        return x[:, :, :28, :28]","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:30:14.103403Z","iopub.execute_input":"2022-06-02T15:30:14.103956Z","iopub.status.idle":"2022-06-02T15:30:14.111492Z","shell.execute_reply.started":"2022-06-02T15:30:14.103918Z","shell.execute_reply":"2022-06-02T15:30:14.110798Z"},"trusted":true},"execution_count":94,"outputs":[]},{"cell_type":"code","source":"class VAE(nn.Module):\n    def __init__(self):\n        super(VAE, self).__init__()\n        '''<определите архитектуры encoder и decoder\n        помните, у encoder должны быть два \"хвоста\", \n        т.е. encoder должен кодировать картинку в 2 переменные -- mu и logsigma>'''\n\n        self.encoder = nn.Sequential(\n                nn.Conv2d(1, 32, stride=(1, 1), kernel_size=(3, 3), padding=1),\n                nn.LeakyReLU(0.01),\n                nn.Conv2d(32, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n                nn.LeakyReLU(0.01),\n                nn.Conv2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n                nn.LeakyReLU(0.01),\n                nn.Conv2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n                nn.LeakyReLU(0.01),\n                nn.Flatten()\n                #nn.BatchNorm2d(3136)\n        )\n        self.z_mean = torch.nn.Linear(3136, 2)\n        self.z_log_var = torch.nn.Linear(3136, 2)\n        #mu = self.z_mean(self.encoder(x))\n        #logsigma = self.z_log_var(self.encoder(x))\n        \n        '''<реализуйте forward проход энкодера\n        в качестве ваозвращаемых переменных -- mu и logsigma>'''\n        \n        self.decoder = nn.Sequential(\n                torch.nn.Linear(2, 3136),\n                Reshape(-1, 64, 7, 7),\n                nn.ConvTranspose2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n                nn.LeakyReLU(0.01),\n                nn.ConvTranspose2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n                nn.LeakyReLU(0.01),\n                nn.ConvTranspose2d(64, 32, stride=(2, 2), kernel_size=(3, 3), padding=0), \n                nn.LeakyReLU(0.01),\n                nn.ConvTranspose2d(32, 1, stride=(1, 1), kernel_size=(3, 3), padding=0), \n                Trim(),  # 1x29x29 -> 1x28x28\n                nn.Flatten(),\n                nn.Linear(784, 512),#########################3333\n                nn.LeakyReLU(0.01),\n                nn.Linear(512, 256),\n                nn.LeakyReLU(0.01),\n                nn.Linear(256, 784),#################################\n                nn.Sigmoid()\n                )\n        #reconstruction = self.decoder(z)\n    \n    def gaussian_sampler(self, mu, logsigma):\n        if self.training:\n            eps = torch.randn(mu.size(0), mu.size(1)).to(device)\n            z = mu + eps * torch.exp(logsigma/2.) \n            return z\n            #<засемплируйте латентный вектор из нормального распределения с параметрами mu и sigma>\n        else:\n            # на инференсе возвращаем не случайный вектор из нормального распределения, а центральный -- mu. \n            # на инференсе выход автоэнкодера должен быть детерминирован.\n            return mu\n    \n\n       #<реализуйте forward проход декодера\n        #в качестве возвращаемой переменной -- reconstruction>\n\n    def forward(self, x):\n        mu, logsigma = self.z_mean(self.encoder(x)), self.z_log_var(self.encoder(x))\n        reconstruction = self.decoder(self.gaussian_sampler(mu, logsigma))#decode(gaussian_sampler(mu, logsigma))\n        #<используя encode и decode, реализуйте forward проход автоэнкодера\n        #в качестве ваозвращаемых переменных -- mu, logsigma и reconstruction>\n        return mu, logsigma, reconstruction","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:30:17.651282Z","iopub.execute_input":"2022-06-02T15:30:17.651580Z","iopub.status.idle":"2022-06-02T15:30:17.670725Z","shell.execute_reply.started":"2022-06-02T15:30:17.651543Z","shell.execute_reply":"2022-06-02T15:30:17.669984Z"},"trusted":true},"execution_count":95,"outputs":[]},{"cell_type":"markdown","source":"Определим лосс и его компоненты для VAE:","metadata":{"id":"hAB77d-PYY76"}},{"cell_type":"markdown","source":"Надеюсь, вы уже прочитали материал в towardsdatascience (или еще где-то) про VAE и знаете, что лосс у VAE состоит из двух частей: KL и log-likelihood.\n\nОбщий лосс будет выглядеть так:\n\n$$\\mathcal{L} = -D_{KL}(q_{\\phi}(z|x)||p(z)) + \\log p_{\\theta}(x|z)$$\n\nФормула для KL-дивергенции:\n\n$$D_{KL} = -\\frac{1}{2}\\sum_{i=1}^{dimZ}(1+log(\\sigma_i^2)-\\mu_i^2-\\sigma_i^2)$$\n\nВ качестве log-likelihood возьмем привычную нам кросс-энтропию.","metadata":{"id":"UxJrkXGQo5bp"}},{"cell_type":"code","source":"def KL_divergence(mu, logsigma):\n\n    loss = - 0.5 * torch.sum(1 + logsigma - mu ** 2 - logsigma.exp()) \n    return loss\n\ndef log_likelihood(x, reconstruction):\n\n    loss = nn.BCELoss(reduction = 'sum')#<binary cross-entropy>\n    return loss(reconstruction, x)\n\ndef loss_vae(x, mu, logsigma, reconstruction):\n    return KL_divergence(mu, logsigma) + log_likelihood(x, reconstruction)\n","metadata":{"id":"ac5ey7uIYY77","execution":{"iopub.status.busy":"2022-06-02T15:42:30.148444Z","iopub.execute_input":"2022-06-02T15:42:30.149017Z","iopub.status.idle":"2022-06-02T15:42:30.154756Z","shell.execute_reply.started":"2022-06-02T15:42:30.148977Z","shell.execute_reply":"2022-06-02T15:42:30.153935Z"},"trusted":true},"execution_count":108,"outputs":[]},{"cell_type":"markdown","source":"И обучим модель:","metadata":{"id":"ZPJQu70eYY79"}},{"cell_type":"code","source":"criterion = loss_vae\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\nautoencoder = VAE().to(device)\n#print(autoencoder)\noptimizer = torch.optim.Adam(autoencoder.parameters(), lr = 0.00007) #0.00005","metadata":{"id":"dtCjfqXdYY79","execution":{"iopub.status.busy":"2022-06-02T15:42:32.457911Z","iopub.execute_input":"2022-06-02T15:42:32.458821Z","iopub.status.idle":"2022-06-02T15:42:32.477758Z","shell.execute_reply.started":"2022-06-02T15:42:32.458773Z","shell.execute_reply":"2022-06-02T15:42:32.477117Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"autoencoder.z_mean(train_loader[0])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epochs = 25\ntrain_losses = []\nval_losses = []\n\nfor epoch in tqdm(range(n_epochs)):\n    autoencoder.train()\n    train_losses_per_epoch = []\n    for batch in train_loader:\n        optimizer.zero_grad()\n        mu, logsigma, reconstruction = autoencoder(torch.tensor(batch[0]).to(device))\n        #print(reconstruction.shape, mu.shape, logsigma.shape, end = '\\n' )\n        reconstruction = reconstruction.view(32,1,28,28)\n        #print(reconstruction.shape)\n        loss = criterion(batch[0].to(device).float(), mu, logsigma, reconstruction)\n        loss.backward()\n        optimizer.step()\n        train_losses_per_epoch.append(loss.item())\n\n    train_losses.append(np.mean(train_losses_per_epoch))\n\n    autoencoder.eval()\n    val_losses_per_epoch = []\n    with torch.no_grad():\n        for batch in val_loader:\n            mu, logsigma, reconstruction = autoencoder(torch.tensor(batch[0]).to(device))\n            #print(reconstruction.shape)\n            reconstruction = reconstruction.view(-1, 1, 28, 28)####################\n            loss = criterion(batch[0].to(device).float(), mu, logsigma, reconstruction)\n            val_losses_per_epoch.append(loss.item())\n\n    val_losses.append(np.mean(val_losses_per_epoch))","metadata":{"scrolled":true,"id":"rY1khca6YY7_","execution":{"iopub.status.busy":"2022-06-02T15:42:34.582208Z","iopub.execute_input":"2022-06-02T15:42:34.582462Z","iopub.status.idle":"2022-06-02T15:52:00.711094Z","shell.execute_reply.started":"2022-06-02T15:42:34.582433Z","shell.execute_reply":"2022-06-02T15:52:00.710339Z"},"trusted":true},"execution_count":110,"outputs":[]},{"cell_type":"code","source":"for ep in tqdm(range(25)):\n    autoencoder.eval()\n    with torch.no_grad():\n        for batch_val in val_loader:\n            mu, logsigma, reconstruction = autoencoder(torch.tensor(batch_val[0]).to(device))\n        #print(batch_val[0].shape)\n            reconstruction = reconstruction.view(-1, 1, 28, 28)####################\n            loss = criterion(batch_val[0].to(device).float(), mu, logsigma, reconstruction)\n            val_losses_per_epoch.append(loss.item())\n\n    val_losses.append(np.mean(val_losses_per_epoch))","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:19:38.846912Z","iopub.execute_input":"2022-05-30T20:19:38.847254Z","iopub.status.idle":"2022-05-30T20:20:20.909409Z","shell.execute_reply.started":"2022-05-30T20:19:38.847212Z","shell.execute_reply":"2022-05-30T20:20:20.908655Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs__ = range(1,26)\nplt.plot(epochs__, train_losses, 'g', label='Training loss')\nplt.plot(epochs__, val_losses, 'b', label='validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:52:10.359128Z","iopub.execute_input":"2022-06-02T15:52:10.359379Z","iopub.status.idle":"2022-06-02T15:52:10.549180Z","shell.execute_reply.started":"2022-06-02T15:52:10.359351Z","shell.execute_reply":"2022-06-02T15:52:10.548462Z"},"trusted":true},"execution_count":111,"outputs":[]},{"cell_type":"markdown","source":"Давайте посмотрим, как наш тренированный VAE кодирует и восстанавливает картинки:","metadata":{"id":"SkxW_8fkYY8B"}},{"cell_type":"code","source":"###################3understand\nautoencoder.eval()\nwith torch.no_grad():\n    for batch in val_loader:\n        encoder_1 = autoencoder.encoder(batch[0].to(device))\n        muu = autoencoder.z_mean(encoder_1)\n        z_log_varr= autoencoder.z_log_var(encoder_1)\n        gaussian = autoencoder.gaussian_sampler(muu, z_log_varr)\n        break","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:52:14.030055Z","iopub.execute_input":"2022-06-02T15:52:14.030314Z","iopub.status.idle":"2022-06-02T15:52:14.044841Z","shell.execute_reply.started":"2022-06-02T15:52:14.030286Z","shell.execute_reply":"2022-06-02T15:52:14.043998Z"},"trusted":true},"execution_count":112,"outputs":[]},{"cell_type":"code","source":"result_val = []\nground_truth_val = []\nautoencoder.eval()\nwith torch.no_grad():\n    for batch in val_loader:\n        mu, logsigma, reconstruction = autoencoder(batch[0].to(device))\n        reconstruction = reconstruction.view(-1, 1, 28, 28)\n        result = reconstruction.cpu().detach().numpy()\n        ground_truth = batch[0].numpy()\n        result_val.extend(result)\n        ground_truth_val.extend(ground_truth)\n        break","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:52:33.729669Z","iopub.execute_input":"2022-06-02T15:52:33.729926Z","iopub.status.idle":"2022-06-02T15:52:33.745315Z","shell.execute_reply.started":"2022-06-02T15:52:33.729898Z","shell.execute_reply":"2022-06-02T15:52:33.744588Z"},"trusted":true},"execution_count":115,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(8, 20))\nfor i, (gt, res) in enumerate(zip(ground_truth_val[:10], result_val[:10])):\n    plt.subplot(10, 2, 2*i+1)\n    plt.imshow(gt[0])\n    plt.subplot(10, 2, 2*i+2)\n    plt.imshow(res[0])","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:52:35.219636Z","iopub.execute_input":"2022-06-02T15:52:35.220332Z","iopub.status.idle":"2022-06-02T15:52:36.579168Z","shell.execute_reply.started":"2022-06-02T15:52:35.220295Z","shell.execute_reply":"2022-06-02T15:52:36.578507Z"},"trusted":true},"execution_count":116,"outputs":[]},{"cell_type":"markdown","source":"Давайте попробуем проделать для VAE то же, что и с обычным автоэнкодером -- подсунуть decoder'у из VAE случайные векторы из нормального распределения и посмотреть, какие картинки получаются:","metadata":{"id":"PQXYIXjoYY8F"}},{"cell_type":"code","source":"# вспомните про замечание из этого же пункта обычного AE про распределение латентных переменных\n#z = np.array([np.random.normal(0, 1, 100) for i in range(10)])\nencod_vae = []\nwith torch.no_grad():\n    for idx, batch in enumerate(val_loader):\n        batch = batch[0]\n        mu, logsigma = autoencoder.z_mean(autoencoder.encoder(batch.to(device).float())), autoencoder.z_log_var(autoencoder.encoder(batch.to(device).float()))\n        reconstruction = autoencoder.gaussian_sampler(mu, logsigma)\n        res_z = reconstruction.cpu().detach().numpy()\n        ground_truth = batch.numpy()\n        encod_vae.extend(res_z)\n        #reck.append(res_z)\nencod_vae = np.array(encod_vae)","metadata":{"id":"bOhhH-osYY8G","execution":{"iopub.status.busy":"2022-06-02T15:54:25.052379Z","iopub.execute_input":"2022-06-02T15:54:25.052662Z","iopub.status.idle":"2022-06-02T15:54:26.385086Z","shell.execute_reply.started":"2022-06-02T15:54:25.052607Z","shell.execute_reply":"2022-06-02T15:54:26.384276Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"encod_vae = []\nwith torch.no_grad():\n    for idx, batch in enumerate(val_loader):\n        batch = batch[0]\n        mu, logsigma = autoencoder.z_mean(autoencoder.encoder(batch.to(device).float())), autoencoder.z_log_var(autoencoder.encoder(batch.to(device).float()))\n        reconstruction = autoencoder.gaussian_sampler(mu, logsigma)\n        res_z = reconstruction.cpu().detach().numpy()\n        ground_truth = batch.numpy()\n        encod_vae.extend(res_z)\nencod_vae = np.array(encod_vae)\n\n\nz_mean_vae = torch.tensor(np.mean([encod_vae[0][i] for i in range(len(encod_vae[0]))]))\nz_std_vae= torch.tensor(np.mean([encod_vae[1][i] for i in range(len(encod_vae[1]))]))\nz_vae = torch.tensor(np.array([np.random.normal(z_mean_vae, abs(2*z_std_vae), 2) for i in range(10)]))\nautoencoder.eval()\nreck = []\ngro = []\nwith torch.no_grad():\n    for i in z_vae:\n        reconstruction = autoencoder.decoder(i.to(device).float())\n        reconstruction = reconstruction.view(1,1, 28, 28)\n        res_z = reconstruction.cpu().detach().numpy()\n        reck.append(res_z)\nlstim = []\nplt.figure(figsize=(8, 20))\nfor i, res in enumerate(reck[:10]):\n    plt.subplot(15, 2, 2*i+1)\n    plt.imshow(res[0][0])","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:55:48.334476Z","iopub.execute_input":"2022-06-02T15:55:48.335156Z","iopub.status.idle":"2022-06-02T15:55:50.451695Z","shell.execute_reply.started":"2022-06-02T15:55:48.335113Z","shell.execute_reply":"2022-06-02T15:55:50.450995Z"},"trusted":true},"execution_count":121,"outputs":[]},{"cell_type":"markdown","source":"## 2.2. Latent Representation (2 балла)","metadata":{"id":"Nzt-ENxCr6ul"}},{"cell_type":"markdown","source":"Давайте посмотрим, как латентные векторы картинок лиц выглядят в пространстве.\nВаша задача -- изобразить латентные векторы картинок точками в двумерном просторанстве. \n\nЭто позволит оценить, насколько плотно распределены латентные векторы изображений цифр в пространстве. \n\nПлюс давайте сделаем такую вещь: покрасим точки, которые соответствуют картинкам каждой цифры, в свой отдельный цвет\n\nПодсказка: красить -- это просто =) У plt.scatter есть параметр c (color), см. в документации.\n\n\nИтак, план:\n1. Получить латентные представления картинок тестового датасета\n2. С помощтю `TSNE` (есть в `sklearn`) сжать эти представления до размерности 2 (чтобы можно было их визуализировать точками в пространстве)\n3. Визуализировать полученные двумерные представления с помощью `matplotlib.scatter`, покрасить разными цветами точки, соответствующие картинкам разных цифр.","metadata":{"id":"uIWy670xr-Uv"}},{"cell_type":"code","source":"tsne_latent = []\nwith torch.no_grad():\n    for batch in val_loader:\n        batch = batch[0]\n        mu, logsigma = autoencoder.z_mean(autoencoder.encoder(batch.to(device))), autoencoder.z_log_var(autoencoder.encoder(batch.to(device)))\n        latent_vector = autoencoder.gaussian_sampler(mu, logsigma)\n        tsne_latent.extend(latent_vector.cpu().detach().numpy())","metadata":{"execution":{"iopub.status.busy":"2022-06-02T15:56:42.030579Z","iopub.execute_input":"2022-06-02T15:56:42.030862Z","iopub.status.idle":"2022-06-02T15:56:43.396221Z","shell.execute_reply.started":"2022-06-02T15:56:42.030832Z","shell.execute_reply":"2022-06-02T15:56:43.395501Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"!pip install tsne-torch\nfrom tsne_torch import TorchTSNE as TSNE","metadata":{"execution":{"iopub.status.busy":"2022-05-28T15:50:04.30528Z","iopub.execute_input":"2022-05-28T15:50:04.305537Z","iopub.status.idle":"2022-05-28T15:50:17.954848Z","shell.execute_reply.started":"2022-05-28T15:50:04.305507Z","shell.execute_reply":"2022-05-28T15:50:17.953992Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nX_embedded_vae = TSNE(n_components=2).fit_transform(tsne_latent)\n#X_embedded = TSNE(n_components=2).fit_transform(encod_vae.T)\n#<ваш код получения латентных представлений, применения TSNE и визуализации>","metadata":{"id":"Bk94C6mCsx9c","execution":{"iopub.status.busy":"2022-06-02T15:59:30.041856Z","iopub.execute_input":"2022-06-02T15:59:30.042549Z","iopub.status.idle":"2022-06-02T16:00:57.646429Z","shell.execute_reply.started":"2022-06-02T15:59:30.042510Z","shell.execute_reply":"2022-06-02T16:00:57.645667Z"},"trusted":true},"execution_count":124,"outputs":[]},{"cell_type":"code","source":"ddd = [test_dataset[i][1] for i in range(len(test_dataset))]","metadata":{"execution":{"iopub.status.busy":"2022-05-30T20:48:22.878354Z","iopub.execute_input":"2022-05-30T20:48:22.878612Z","iopub.status.idle":"2022-05-30T20:48:23.652932Z","shell.execute_reply.started":"2022-05-30T20:48:22.878583Z","shell.execute_reply":"2022-05-30T20:48:23.6522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport seaborn as sns\nd_vae = [test_dataset[i][1] for i in range(len(test_dataset))]\ndf = pd.DataFrame()\ndf[\"y\"] = d_vae\ndf[\"xx\"] = X_embedded_vae[:,0]\ndf[\"yy\"] = X_embedded_vae[:,1]\n\nsns.scatterplot(x=\"xx\", y=\"yy\", hue=df.y.tolist(),\n                palette=sns.color_palette(\"hls\", 10),\n                data=df).set(title=\"MNIST data T-SNE projection\")\n ","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:02:38.499639Z","iopub.execute_input":"2022-06-02T16:02:38.499906Z","iopub.status.idle":"2022-06-02T16:02:40.176162Z","shell.execute_reply.started":"2022-06-02T16:02:38.499877Z","shell.execute_reply":"2022-06-02T16:02:40.175485Z"},"trusted":true},"execution_count":127,"outputs":[]},{"cell_type":"markdown","source":"Что вы думаете о виде латентного представления?","metadata":{"id":"ifxhsvPss5h_"}},{"cell_type":"markdown","source":"__Congrats v2.0!__","metadata":{"id":"ESPBHrL3YY8H"}},{"cell_type":"markdown","source":"## 2.3. Conditional VAE (6 баллов)\n","metadata":{"id":"yIYuKFwijN2U"}},{"cell_type":"markdown","source":"Мы уже научились обучать обычный AE на датасете картинок и получать новые картинки, используя генерацию шума и декодер. \nДавайте теперь допустим, что мы обучили AE на датасете MNIST и теперь хотим генерировать новые картинки с числами с помощью декодера (как выше мы генерили рандомные лица). \nИ вот нам понадобилось сгенерировать цифру 8, и мы подставляем разные варианты шума, но восьмерка никак не генерится:(\n\nХотелось бы добавить к нашему AE функцию \"выдай мне рандомное число из вот этого вот класса\", где классов десять (цифры от 0 до 9 образуют десять классов).  Conditional AE — так называется вид автоэнкодера, который предоставляет такую возможность. Ну, название \"conditional\" уже говорит само за себя.\n\nИ в этой части задания мы научимся такие обучать.","metadata":{"id":"c5l8Bu1RPjUx"}},{"cell_type":"markdown","source":"### Архитектура\n\nНа картинке ниже представлена архитектура простого Conditional VAE.\n\nПо сути, единственное отличие от обычного -- это то, что мы вместе с картинкой в первом слое энкодера и декодера передаем еще информацию о классе картинки. \n\nТо есть, в первый (входной) слой энкодера подается конкатенация картинки и информации о классе (например, вектора из девяти нулей и одной единицы). В первый слой декодера подается конкатенация латентного вектора и информации о классе.","metadata":{"id":"0j8zNIwKPY-6"}},{"cell_type":"markdown","source":"\n![alt text](https://sun9-63.userapi.com/impg/Mh1akf7mfpNoprrSWsPOouazSmTPMazYYF49Tw/djoHNw_9KVA.jpg?size=1175x642&quality=96&sign=e88baec5f9bb91c8443fba31dcf0a4df&type=album)\n\n![alt text](https://sun9-73.userapi.com/impg/UDuloLNKhzTBYAKewgxke5-YPsAKyGOqA-qCRg/MnyCavJidxM.jpg?size=1229x651&quality=96&sign=f2d21bfacc1c5755b76868dc4cfef39c&type=album)\n\n","metadata":{"id":"Y6YloFEAPeM4"}},{"cell_type":"markdown","source":"На всякий случай: это VAE, то есть, latent у него все еще состоит из mu и sigma","metadata":{"id":"hxg2tDSfRbLF"}},{"cell_type":"markdown","source":"Таким образом, при генерации новой рандомной картинки мы должны будем передать декодеру сконкатенированные латентный вектор и класс картинки.","metadata":{"id":"GpFbSXLaPrm1"}},{"cell_type":"markdown","source":"P.S. Также можно передавать класс картинки не только в первый слой, но и в каждый слой сети. То есть на каждом слое конкатенировать выход из предыдущего слоя и информацию о классе.","metadata":{"id":"cX0zxklMPwI2"}},{"cell_type":"code","source":"\nclass CVAE(nn.Module):\n    def __init__(self):\n        super(CVAE, self).__init__()\n        '''<определите архитектуры encoder и decoder\n        помните, у encoder должны быть два \"хвоста\", \n        т.е. encoder должен кодировать картинку в 2 переменные -- mu и logsigma>'''\n\n        self.encoder = nn.Sequential(\n                nn.Conv2d(1, 32, stride=(1, 1), kernel_size=(3, 3), padding=1),\n                nn.LeakyReLU(0.01),\n                nn.Conv2d(32, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n                nn.LeakyReLU(0.01),\n                nn.Conv2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),\n                nn.LeakyReLU(0.01),\n                nn.Conv2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n                nn.Flatten(),\n        )\n        self.z_mean = torch.nn.Linear(3136, 2)\n        self.z_log_var = torch.nn.Linear(3136, 2)\n        #mu = self.z_mean(self.encoder(x))\n        #logsigma = self.z_log_var(self.encoder(x))\n        \n        '''<реализуйте forward проход энкодера\n        в качестве ваозвращаемых переменных -- mu и logsigma>'''\n        \n        self.decoder = nn.Sequential(\n                torch.nn.Linear(12, 3136),\n                Reshape(-1, 64, 7, 7),\n                nn.ConvTranspose2d(64, 64, stride=(1, 1), kernel_size=(3, 3), padding=1),\n                nn.LeakyReLU(0.01),\n                nn.ConvTranspose2d(64, 64, stride=(2, 2), kernel_size=(3, 3), padding=1),                \n                nn.LeakyReLU(0.01),\n                nn.ConvTranspose2d(64, 32, stride=(2, 2), kernel_size=(3, 3), padding=0),                \n                nn.LeakyReLU(0.01),\n                nn.ConvTranspose2d(32, 1, stride=(1, 1), kernel_size=(3, 3), padding=0), \n                Trim(),  # 1x29x29 -> 1x28x28\n\n                nn.Sigmoid()\n                )\n        #reconstruction = self.decoder(z)\n    \n    def gaussian_sampler(self, mu, logsigma):\n        if self.training:\n            eps = torch.randn(mu.size(0), mu.size(1)).to(device)\n            z = mu + eps * torch.exp(logsigma/2.) \n            return z\n            #<засемплируйте латентный вектор из нормального распределения с параметрами mu и sigma>\n        else:\n            # на инференсе возвращаем не случайный вектор из нормального распределения, а центральный -- mu. \n            # на инференсе выход автоэнкодера должен быть детерминирован.\n            return mu\n    \n    def one_hot(self, labels):\n        targets = torch.zeros(labels.size(0), 10)\n        for i, label in enumerate(labels):\n            targets[i, label] = 1\n        return targets.to(device)\n       #<реализуйте forward проход декодера\n        #в качестве возвращаемой переменной -- reconstruction>\n\n    def forward(self, x, labels):\n        mu, logsigma = self.z_mean(self.encoder(x)), self.z_log_var(self.encoder(x))\n        gaus_sample = self.gaussian_sampler(mu, logsigma)\n        one_hot_vector = self.one_hot(labels)\n        reconstruction = self.decoder(torch.cat([gaus_sample, one_hot_vector], 1))#decode(gaussian_sampler(mu, logsigma))\n        #<используя encode и decode, реализуйте forward проход автоэнкодера\n        #в качестве ваозвращаемых переменных -- mu, logsigma и reconstruction>\n        return mu, logsigma, reconstruction","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:02:58.769495Z","iopub.execute_input":"2022-06-02T16:02:58.770132Z","iopub.status.idle":"2022-06-02T16:02:58.787956Z","shell.execute_reply.started":"2022-06-02T16:02:58.770097Z","shell.execute_reply":"2022-06-02T16:02:58.787054Z"},"trusted":true},"execution_count":128,"outputs":[]},{"cell_type":"code","source":"criterion = loss_vae\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\nautoencoder = CVAE().to(device)\n#print(autoencoder)\noptimizer = torch.optim.Adam(autoencoder.parameters(), lr = 0.00005)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:03:02.660783Z","iopub.execute_input":"2022-06-02T16:03:02.661319Z","iopub.status.idle":"2022-06-02T16:03:02.677108Z","shell.execute_reply.started":"2022-06-02T16:03:02.661279Z","shell.execute_reply":"2022-06-02T16:03:02.676415Z"},"trusted":true},"execution_count":129,"outputs":[]},{"cell_type":"code","source":"n_epochs = 25\ntrain_losses = []\nval_losses = []\n#train_losses_per_epoch = []\n#val_losses_per_epoch = []\nfor epoch in tqdm(range(n_epochs)):\n    autoencoder.train()\n    train_losses_per_epoch = []\n    for i, (batch, labels) in enumerate(train_loader):\n        optimizer.zero_grad()\n        #print(type(batch))\n        mu, logsigma, reconstruction = autoencoder(torch.tensor(batch).to(device), torch.tensor(labels).to(device))\n        #print(reconstruction.shape, mu.shape, logsigma.shape, end = '\\n' )\n        #reconstruction = reconstruction.view(32,1,28,28)#(-1, 28, 28, 3)\n        loss = criterion(batch.to(device).float(), mu, logsigma, reconstruction)\n        loss.backward()\n        optimizer.step()\n        train_losses_per_epoch.append(loss.item())\n    #train_losses.append(train_losses_per_epoch/ 32)\n    train_losses.append(np.mean(train_losses_per_epoch))\n\n    autoencoder.eval()\n    val_losses_per_epoch = []\n    with torch.no_grad():\n        for i, (batch, label) in enumerate(val_loader):\n            mu, logsigma, reconstruction = autoencoder(torch.tensor(batch).to(device), torch.tensor(label).to(device))\n            #reconstruction = reconstruction.view(-1,1,28,28)\n            loss = criterion(batch.to(device).float(), mu, logsigma, reconstruction)\n            val_losses_per_epoch.append(loss.item())\n\n    val_losses.append(np.mean(val_losses_per_epoch))","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:03:07.366849Z","iopub.execute_input":"2022-06-02T16:03:07.367112Z","iopub.status.idle":"2022-06-02T16:12:25.182686Z","shell.execute_reply.started":"2022-06-02T16:03:07.367082Z","shell.execute_reply":"2022-06-02T16:12:25.181956Z"},"trusted":true},"execution_count":130,"outputs":[]},{"cell_type":"code","source":"epochs__ = range(1,26)\nplt.plot(epochs__, train_losses, 'g', label='Training loss')\nplt.plot(epochs__, val_losses, 'b', label='validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:12:29.709642Z","iopub.execute_input":"2022-06-02T16:12:29.710276Z","iopub.status.idle":"2022-06-02T16:12:29.886124Z","shell.execute_reply.started":"2022-06-02T16:12:29.710241Z","shell.execute_reply":"2022-06-02T16:12:29.885470Z"},"trusted":true},"execution_count":131,"outputs":[]},{"cell_type":"markdown","source":"### Sampling\n","metadata":{"id":"VoMw-IFyP5A2"}},{"cell_type":"markdown","source":"Тут мы будем сэмплировать из CVAE. Это прикольнее, чем сэмплировать из простого AE/VAE: тут можно взять один и тот же латентный вектор и попросить CVAE восстановить из него картинки разных классов!\nДля MNIST вы можете попросить CVAE восстановить из одного латентного вектора, например, картинки цифры 5 и 7.","metadata":{"id":"qe1zWyZHkLV2"}},{"cell_type":"code","source":"cvae_latent = []\nwith torch.no_grad():\n    for i, (batch, labels) in enumerate(train_loader):\n        batch = batch.to(device)\n        mu, logsigma = autoencoder.z_mean(autoencoder.encoder(batch)), autoencoder.z_log_var(autoencoder.encoder(batch))\n        cvae_lat = autoencoder.gaussian_sampler(mu, logsigma)\n        cvae_latent.extend(cvae_lat)\n        \n        \n'''<тут нужно научиться сэмплировать из декодера цифры определенного класса>'''","metadata":{"id":"A0SQIhvNP9Dr","execution":{"iopub.status.busy":"2022-06-02T16:12:51.243261Z","iopub.execute_input":"2022-06-02T16:12:51.243509Z","iopub.status.idle":"2022-06-02T16:12:58.728068Z","shell.execute_reply.started":"2022-06-02T16:12:51.243481Z","shell.execute_reply":"2022-06-02T16:12:58.725972Z"},"trusted":true},"execution_count":132,"outputs":[]},{"cell_type":"code","source":"cvae_random = np.random.choice(60000,15)\nprint(cvae_random)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:13:04.986436Z","iopub.execute_input":"2022-06-02T16:13:04.986711Z","iopub.status.idle":"2022-06-02T16:13:04.992250Z","shell.execute_reply.started":"2022-06-02T16:13:04.986682Z","shell.execute_reply":"2022-06-02T16:13:04.991246Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"one_5 = torch.tensor([0,0,0,0,0,1,0,0,0,0]).to(device)\none_7 = torch.tensor([0,0,0,0,0,0,0,1,0,0]).to(device)\nsave_5, save_7 = [], []\nwith torch.no_grad():\n    for i in cvae_random:\n        recon_5 = autoencoder.decoder(torch.cat([cvae_latent[i] , one_5]))\n        recon_7 = autoencoder.decoder(torch.cat([cvae_latent[i] , one_7]))\n        save_5.extend(recon_5.cpu().detach().numpy())\n        save_7.extend(recon_7.cpu().detach().numpy())","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:13:17.420603Z","iopub.execute_input":"2022-06-02T16:13:17.421310Z","iopub.status.idle":"2022-06-02T16:13:17.447501Z","shell.execute_reply.started":"2022-06-02T16:13:17.421274Z","shell.execute_reply":"2022-06-02T16:13:17.446826Z"},"trusted":true},"execution_count":134,"outputs":[]},{"cell_type":"code","source":"save_5, save_7 = np.array(save_5), np.array(save_7)\nplt.figure(figsize=(8, 20))\nfor i, (res, gt) in enumerate(zip(save_5[:15], save_7[:15])):\n    plt.subplot(15, 2, 2*i+1)\n    plt.imshow(gt[0])\n    plt.subplot(15, 2, 2*i+2)\n    plt.imshow(res[0])\n","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:13:20.005534Z","iopub.execute_input":"2022-06-02T16:13:20.006098Z","iopub.status.idle":"2022-06-02T16:13:22.620119Z","shell.execute_reply.started":"2022-06-02T16:13:20.006063Z","shell.execute_reply":"2022-06-02T16:13:22.609911Z"},"trusted":true},"execution_count":135,"outputs":[]},{"cell_type":"markdown","source":"Splendid! Вы великолепны!\n","metadata":{"id":"nAWBu8rzQBgQ"}},{"cell_type":"markdown","source":"### Latent Representations","metadata":{"id":"Rt2S77cm3O1v"}},{"cell_type":"markdown","source":"Давайте посмотрим, как выглядит латентное пространство картинок в CVAE и сравним с картинкой для VAE =)\n\nОпять же, нужно покрасить точки в разные цвета в зависимости от класса.","metadata":{"id":"Nt7x8Ek_rHTE"}},{"cell_type":"code","source":"tsne_latent_cvae = []\nwith torch.no_grad():\n    for i, (batch, labels) in enumerate(val_loader):\n        batch, labels = batch.to(device), labels.to(device)\n        mu, logsigma = autoencoder.z_mean(autoencoder.encoder(batch.to(device))), autoencoder.z_log_var(autoencoder.encoder(batch.to(device)))\n        latent_vector = autoencoder.gaussian_sampler(mu, logsigma)\n        #print(latent_vector.size(), autoencoder.one_hot(labels).size())\n        #cat_cvae = torch.cat([latent_vector, autoencoder.one_hot(labels)], 1)\n        tsne_latent_cvae.extend(latent_vector.cpu().detach().numpy())","metadata":{"id":"LSCYK7sH3KEc","execution":{"iopub.status.busy":"2022-06-02T16:26:32.759096Z","iopub.execute_input":"2022-06-02T16:26:32.759771Z","iopub.status.idle":"2022-06-02T16:26:34.213478Z","shell.execute_reply.started":"2022-06-02T16:26:32.759730Z","shell.execute_reply":"2022-06-02T16:26:34.212777Z"},"trusted":true},"execution_count":139,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\nX_embedded_cvae = TSNE(n_components=2).fit_transform(tsne_latent_cvae)","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:26:35.827279Z","iopub.execute_input":"2022-06-02T16:26:35.827551Z","iopub.status.idle":"2022-06-02T16:28:00.630293Z","shell.execute_reply.started":"2022-06-02T16:26:35.827521Z","shell.execute_reply":"2022-06-02T16:28:00.628924Z"},"trusted":true},"execution_count":140,"outputs":[]},{"cell_type":"code","source":"d_cvae = [test_dataset[i][1] for i in range(len(test_dataset))]\nimport pandas as pd\nimport seaborn as sns\ndf = pd.DataFrame()\ndf[\"y\"] = d_cvae\ndf[\"comp-1\"] = X_embedded_cvae[:,0]\ndf[\"comp-2\"] = X_embedded_cvae[:,1]\n\nsns.scatterplot(x=\"comp-1\", y=\"comp-2\", hue=df.y.tolist(),\n                palette=sns.color_palette(\"hls\", 10),\n                data=df).set(title=\"MNIST data T-SNE projection\")","metadata":{"execution":{"iopub.status.busy":"2022-06-02T16:30:45.048670Z","iopub.execute_input":"2022-06-02T16:30:45.049345Z","iopub.status.idle":"2022-06-02T16:30:46.978148Z","shell.execute_reply.started":"2022-06-02T16:30:45.049308Z","shell.execute_reply":"2022-06-02T16:30:46.977489Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"markdown","source":"Что вы думаете насчет этой картинки? Отличается от картинки для VAE?","metadata":{"id":"ET8IELWu3Z2c"}},{"cell_type":"markdown","source":"","metadata":{"id":"SWkqHjvTCD_8"}},{"cell_type":"markdown","source":"# BONUS 1: Denoising\n\n## Внимание! За бонусы доп. баллы не ставятся, но вы можете сделать их для себя.","metadata":{"id":"KN3D_k5W_WZz"}},{"cell_type":"markdown","source":"У автоэнкодеров, кроме сжатия и генерации изображений, есть другие практические применения. Про одно из них эта бонусная часть задания.","metadata":{"id":"12a1jkpkCsIU"}},{"cell_type":"markdown","source":"Автоэнкодеры могут быть использованы для избавления от шума на фотографиях (denoising). Для этого их нужно обучить специальным образом: input картинка будет зашумленной, а выдавать автоэнкодер должен будет картинку без шума. \nТо есть, loss-функция AE останется той же (MSE между реальной картинкой и выданной), а на вход автоэнкодеру будет подаваться зашумленная картинка.","metadata":{"id":"v8EN-8jlCtmd"}},{"cell_type":"markdown","source":"<a href=\"https://ibb.co/YbRJ1nZ\"><img src=\"https://i.ibb.co/0QD164t/Screen-Shot-2020-06-04-at-4-49-50-PM.png\" alt=\"Screen-Shot-2020-06-04-at-4-49-50-PM\" border=\"0\"></a>","metadata":{"id":"j1OJg6jhlaZl"}},{"cell_type":"markdown","source":"Для этого нужно взять ваш любимый датасет (датасет лиц из первой части этого задания или любой другой) и сделать копию этого датасета с шумом. \n\nВ питоне шум можно добавить так:","metadata":{"id":"ysI0BCuRDbvm"}},{"cell_type":"code","source":"noise_factor = 0.5\nX_noisy = X + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=X.shape) ","metadata":{"id":"X5e746iVDgSm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"<тут ваш код обучения автоэнкодера на зашумленных картинках. Не забудтье разбить на train/test!>","metadata":{"id":"9fSPkXMtDpd5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"<тут проверка, как AE убирает щум с тестовых картинок. Надеюсь, все получилось =)>","metadata":{"id":"B03NQ_sKDvg2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# BONUS 2: Image Retrieval\n\n## Внимание! За бонусы доп. баллы не ставятся, но вы можете сделать их для себя.","metadata":{"id":"-NDiCPYLm2bY"}},{"cell_type":"markdown","source":"Давайте представим, что весь наш тренировочный датасет -- это большая база данных людей. И вот мы получили картинку лица какого-то человека с уличной камеры наблюдения (у нас это картинка из тестового датасета) и хотим понять, что это за человек. Что нам делать? Правильно -- берем наш VAE, кодируем картинку в латентное представление и ищем среди латентныз представлений лиц нашей базы самые ближайшие!","metadata":{"id":"xao_27WMm7AL"}},{"cell_type":"markdown","source":"План:\n\n1. Получаем латентные представления всех лиц тренировочного датасета\n2. Обучаем на них LSHForest `(sklearn.neighbors.LSHForest)`, например, с `n_estimators=50`\n3. Берем картинку из тестового датасета, с помощью VAE получаем ее латентный вектор\n4. Ищем с помощью обученного LSHForest ближайшие из латентных представлений тренировочной базы\n5. Находим лица тренировочного датасета, которым соответствуют ближайшие латентные представления, визуализируем!","metadata":{"id":"Y__bdS23ndeY"}},{"cell_type":"markdown","source":"Немного кода вам в помощь: (feel free to delete everything and write your own)","metadata":{"id":"IksC2ucIoND-"}},{"cell_type":"code","source":"codes = <поучите латентные представления картинок из трейна>","metadata":{"id":"hK0YpLMRoEa0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# обучаем LSHForest\nfrom sklearn.neighbors import LSHForest\nlshf = LSHForest(n_estimators=50).fit(codes)","metadata":{"id":"KisDrgZdoWdt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_similar(image, n_neighbors=5):\n  # функция, которая берет тестовый image и с помощью метода kneighbours у LSHForest ищет ближайшие векторы\n  # прогоняет векторы через декодер и получает картинки ближайших людей\n\n  code = <получение латентного представления image>\n    \n  (distances,),(idx,) = lshf.kneighbors(code, n_neighbors=n_neighbors)\n\n  return distances, X_train[idx]","metadata":{"id":"Y_S5zPb5obam"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show_similar(image):\n\n  # функция, которая принимает тестовый image, ищет ближайшие к нему и визуализирует результат\n    \n    distances,neighbors = get_similar(image,n_neighbors=11)\n    \n    plt.figure(figsize=[8,6])\n    plt.subplot(3,4,1)\n    plt.imshow(image.cpu().numpy().transpose([1,2,0]))\n    plt.title(\"Original image\")\n    \n    for i in range(11):\n        plt.subplot(3,4,i+2)\n        plt.imshow(neighbors[i].cpu().numpy().transpose([1,2,0]))\n        plt.title(\"Dist=%.3f\"%distances[i])\n    plt.show()","metadata":{"id":"t2kjV5wupLP_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"<тут выведите самые похожие лица к какому-нибудь лицу из тестовой части датасета>","metadata":{"id":"w3Ja1UNf_oJq"},"execution_count":null,"outputs":[]}]}